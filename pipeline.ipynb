{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6d87ca",
   "metadata": {},
   "source": [
    "# Google Colab Turtle Recognition Pipeline - Tag a Turtle\n",
    "## 1. Introduction\n",
    "The goal of this pipeline is to identify sea turtles by their facial features using machine learning models. It processes turtle face images, creates embeddings for these faces, and matches them to known turtle identities.\n",
    "\n",
    "## 2. Requirements\n",
    "* Environment: Google Colab\n",
    "\n",
    "* Inputs:\n",
    "A gallery of turtle images (either flat or by identity).\n",
    "A query folder with turtle images for identification.\n",
    "\n",
    "* Outputs:\n",
    "A gallery per query image showing ranked turtle images by similarity.\n",
    "\n",
    "* Modularity: \n",
    "The pipeline is designed to be modular, allowing easy swapping of models and processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29249256",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "### Process configuration\n",
    "Parameters that impact recognition results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5f1030",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The resized image to center crop from for the embedding model\n",
    "resize_scale = 256\n",
    "\n",
    "# Resize scale before passing to Detection model\n",
    "full_resize_n = 1920\n",
    "\n",
    "# Combine all images of a particular turtle from the database into one for improved performance\n",
    "aggregate_dataset_identities = False\n",
    "\n",
    "# Combine all query images of the same turtle into one for improved performance\n",
    "aggregate_query_identities = False\n",
    "\n",
    "# The amount of most similar images to be shown (top 10, or top 5 most similar turtles)\n",
    "top_n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a278c",
   "metadata": {},
   "source": [
    "### Parallelism configuration\n",
    "Options that impact system speed and resource usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b03d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger batch sizes speed up processing but may overflow memory.\n",
    "inference_batch_size = 8\n",
    "\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ec4b3",
   "metadata": {},
   "source": [
    "### Paths configuration\n",
    "Only for developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9656d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/delta/Documents/Turtles/dataset_May15th/train/reid\"\n",
    "yolo_checkpoint_path = '/home/delta/Documents/Turtles/dataset_ops/Tag-A-Turtle/weights/best.pt'\n",
    "resnet101_checkpoint_path = \"/home/delta/Documents/Turtles/Proxy-Anchor/logs/best_wobbly-pond-226.pth\"\n",
    "query_folder_path = \"/home/delta/Documents/Turtles/Proxy-Anchor/query_images\"\n",
    "\n",
    "# Path of the embedded dataset. This embedding file has to be regenerated if the model is updated.\n",
    "saved_embeddings_path = \"./RecognitionCache/embeddings/embeddings.pth\"\n",
    "# Path of the generated metadata\n",
    "auto_metadata_path = f\"/home/delta/Documents/Turtles/Proxy-Anchor/RecognitionCache/datasets/auto_{dataset_path.split('/')[-1]}.csv\"\n",
    "metadata_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2c44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "error_crops = 0\n",
    "def set_error_crops(n):\n",
    "    global error_crops\n",
    "    error_crops = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b416833",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58828243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "from torchvision.models import resnet101\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.data.loaders import LoadTensor\n",
    "\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.data.loaders import LoadTensor\n",
    "import re\n",
    "import csv\n",
    "# determine device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c68a98",
   "metadata": {},
   "source": [
    "## Set the device on which to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d205147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the device to 'cuda' if a GPU is available, otherwise default to 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = device if use_gpu else \"cpu\"\n",
    "if use_gpu and device == \"cpu\":\n",
    "    print(\"GPU not available.\")\n",
    "# Verify the device being used\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7c7ed",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c63a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(file_path):\n",
    "    # Extract the filename from the full file path\n",
    "    filename = os.path.basename(file_path)\n",
    "    \n",
    "    # Check for format 1: \"03_066_R_2003_1.jpg\" (any extension, underscores included)\n",
    "    match1 = re.match(r\"(\\d{2}[-_]\\d{3})_([A-Za-z])_\\d{4}_\\d+\\.[a-zA-Z]+$\", filename)\n",
    "    if match1:\n",
    "        return [match1.group(1), match1.group(2)]\n",
    "    \n",
    "    # Check for format 2: \"03_066 R.JPG\" (any extension, underscores included)\n",
    "    match2 = re.match(r\"(\\d{2}[-_]\\d{3})\\s([A-Za-z])\\.[a-zA-Z]+$\", filename)\n",
    "    if match2:\n",
    "        return [match2.group(1), match2.group(2)]\n",
    "    \n",
    "    # Return None if no match found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bee423ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./RecognitionCache/datasets\", exist_ok=True)\n",
    "os.makedirs(\"./RecognitionCache/embeddings\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84625b",
   "metadata": {},
   "source": [
    "## Generating a metadata for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e7947da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_folder_metadata(root_path):\n",
    "    file_paths = []\n",
    "    \n",
    "    # Walk through the root directory and its subdirectories\n",
    "    for subdir, _, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            # Join the subdirectory and file name to get the full path\n",
    "            file_paths.append(os.path.join(subdir, file))\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        labels = extract_labels(file_path)    \n",
    "        # Add the row to the new format data\n",
    "        data.append({\n",
    "            'bonaire_turtle_id': labels[0],\n",
    "            'side': labels[1],\n",
    "            'filename': file_path,\n",
    "        })\n",
    "    df = pd.DataFrame(data)    \n",
    "    df.to_csv(auto_metadata_path, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e6ae8",
   "metadata": {},
   "source": [
    "## Generating metadata for the query files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbfb09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_folder_metadata(root_path):\n",
    "    data = []\n",
    "    \n",
    "    # Walk through the root directory and its subdirectories\n",
    "    for subdir, _, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            # Join the subdirectory and file name to get the full path\n",
    "            data.append({\n",
    "                'bonaire_turtle_id': f\"{subdir}\",\n",
    "                'side': \"U\",\n",
    "                'filename': os.path.join(subdir, file),\n",
    "            })\n",
    "    df = pd.DataFrame(data)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf0522",
   "metadata": {},
   "source": [
    "## Make sure there is metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faae87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_metadata():\n",
    "    data_path = None\n",
    "    \n",
    "    # Try loading user provided metadata\n",
    "    if metadata_path:\n",
    "        data_path = metadata_path\n",
    "        \n",
    "    # If it fails, load auto metadata\n",
    "    else:\n",
    "        data_path = auto_metadata_path\n",
    "    data_exists = os.path.isfile(data_path)\n",
    "    \n",
    "    # If there is no metadata, generate it\n",
    "    if not data_exists:\n",
    "        print(f\"There is no metadata file {data_path}\")\n",
    "        generate_data_folder_metadata(dataset_path)\n",
    "        data_path = auto_metadata_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a629559f",
   "metadata": {},
   "source": [
    "## Resize image tensor batch from any size to size N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c672224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images_to_n_padded(image_batch, n=224):\n",
    "    \"\"\"\n",
    "    Takes a batch of image tensors, resizes the smaller dimension to 'n', and pads \n",
    "    the larger dimension with black to fit a square of size n x n. Returns a batch of \n",
    "    images in tensor form.\n",
    "\n",
    "    Args:\n",
    "        image_batch (torch.Tensor): A tensor batch of images with shape (B, C, H, W), where B is the batch size, \n",
    "                                   C is the number of channels (3 for RGB), H is the height, and W is the width.\n",
    "        n (int): The target size for the square image (default 224).\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor batch of images, each of size n x n.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((n, n)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    batch_images = []\n",
    "    for img_tensor in image_batch:\n",
    "        img = transforms.ToPILImage()(img_tensor)\n",
    "\n",
    "        width, height = img.size\n",
    "        if width == height:\n",
    "            # If the image is already square, simply resize\n",
    "            transformed_image = transform(img)\n",
    "        elif width < height:\n",
    "            # Resize height to n, calculate padding for width\n",
    "            new_height = n\n",
    "            new_width = int(width * (new_height / height))\n",
    "            padding_left = (n - new_width) // 2\n",
    "            padding_right = n - new_width - padding_left\n",
    "            transformed_image = transforms.Compose([\n",
    "                transforms.Resize((new_height, new_width)),\n",
    "                transforms.Pad((padding_left, 0, padding_right, 0), fill=0, padding_mode='constant'),\n",
    "                transforms.ToTensor()\n",
    "            ])(img) \n",
    "        else:  # height < width\n",
    "            # Resize width to n, calculate padding for height\n",
    "            new_width = n\n",
    "            new_height = int(height * (new_width / width))\n",
    "            padding_top = (n - new_height) // 2\n",
    "            padding_bottom = n - new_height - padding_top\n",
    "            transformed_image = transforms.Compose([\n",
    "                transforms.Resize((new_height, new_width)),\n",
    "                transforms.Pad((0, padding_top, 0, padding_bottom), fill=0, padding_mode='constant'),\n",
    "                transforms.ToTensor()\n",
    "            ])(img)\n",
    "\n",
    "        batch_images.append(transformed_image)\n",
    "\n",
    "    # Stack images into a single tensor batch\n",
    "    return torch.stack(batch_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26222d2b",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4ac280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TurtlesDataset(Dataset):\n",
    "    def __init__(self, mode: str, root: str = None, transform=None, ignoreThreshold=0, data=None):\n",
    "        self.mode = mode.lower()\n",
    "        self.transform = transform\n",
    "        data_path = metadata_path if metadata_path else auto_metadata_path\n",
    "        if not data:\n",
    "            self.root = root\n",
    "            meta_path = os.path.join(self.root, data_path)\n",
    "            if not os.path.isfile(meta_path):\n",
    "                raise FileNotFoundError(f\"No metadata file found (expected {data_path}).\")\n",
    "            data_df = pd.read_csv(meta_path)\n",
    "        else:\n",
    "            data_df = data\n",
    "            \n",
    "        print(f\"Dataset size: {len(data_df)}\")\n",
    "        grouped = data_df.groupby(['bonaire_turtle_id', 'side'])\n",
    "        group_counts = grouped.size()\n",
    "\n",
    "        if ignoreThreshold > 0: \n",
    "            multi_sample_groups = group_counts[group_counts > ignoreThreshold]\n",
    "            print(\"Removing single-sample classes for BonaireTurtlesDataset.\")\n",
    "        else: multi_sample_groups = group_counts\n",
    "\n",
    "        self.im_paths, self._y_strs, self.positions = [], [], []\n",
    "        for (turtle_id, side), group_df in grouped:\n",
    "            if (turtle_id, side) not in multi_sample_groups.index:\n",
    "                continue\n",
    "\n",
    "            for _, row in group_df.iterrows():\n",
    "                filename = row.get(\"filename\", \"\").strip()\n",
    "                if not filename:\n",
    "                    continue\n",
    "\n",
    "                img_path = filename\n",
    "                if not os.path.isfile(img_path):\n",
    "                    continue\n",
    "\n",
    "                identity = f\"{turtle_id}\"\n",
    "                self.im_paths.append(img_path)\n",
    "                self._y_strs.append(identity)\n",
    "                self.positions.append(side)\n",
    "\n",
    "        if not self.im_paths:\n",
    "            raise RuntimeError(\"Dataset is empty.\")\n",
    "\n",
    "        all_classes = sorted(set(self._y_strs))\n",
    "\n",
    "        filtered = [i for i, lbl in enumerate(self._y_strs) if lbl in all_classes]\n",
    "        self.im_paths = [self.im_paths[i] for i in filtered]\n",
    "        self._y_strs = [self._y_strs[i] for i in filtered]\n",
    "\n",
    "        self.classes = sorted(all_classes)\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        self.ys = [self.class_to_idx[s] for s in self._y_strs]\n",
    "        print(\"Data length:\", len(self.ys))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.im_paths[index]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img).unsqueeze(0)\n",
    "            img = resize_images_to_n_padded(img, full_resize_n).squeeze(0)\n",
    "        target = self.ys[index]\n",
    "        return img, target\n",
    "    \n",
    "    def get_path(self, index):\n",
    "        return self.im_paths[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8269d1",
   "metadata": {},
   "source": [
    "## Get Labels from Dataset/DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9832a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_labels(dataloader):\n",
    "    \"\"\"\n",
    "    This function takes a PyTorch DataLoader and returns a tensor containing all the labels.\n",
    "    \n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): A DataLoader object containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing all the labels.\n",
    "    \"\"\"\n",
    "    all_labels = []\n",
    "\n",
    "    for _, labels in dataloader:\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # Concatenate all labels into a single tensor\n",
    "    return torch.cat(all_labels, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0bee72",
   "metadata": {},
   "source": [
    "## Resnet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aed3175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Resnet101(nn.Module):\n",
    "    def __init__(self,embedding_size, pretrained=True, is_norm=True, bn_freeze = True):\n",
    "        super(Resnet101, self).__init__()\n",
    "\n",
    "        self.model = resnet101(pretrained)\n",
    "        self.is_norm = is_norm\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_ftrs = self.model.fc.in_features\n",
    "        self.model.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.model.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.model.embedding = nn.Linear(self.num_ftrs, self.embedding_size)\n",
    "        self._initialize_weights()\n",
    "\n",
    "        if bn_freeze:\n",
    "            for m in self.model.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    m.eval()\n",
    "                    m.weight.requires_grad_(False)\n",
    "                    m.bias.requires_grad_(False)\n",
    "\n",
    "    def l2_norm(self,input):\n",
    "        input_size = input.size()\n",
    "        buffer = torch.pow(input, 2)\n",
    "\n",
    "        normp = torch.sum(buffer, 1).add_(1e-12)\n",
    "        norm = torch.sqrt(normp)\n",
    "\n",
    "        _output = torch.div(input, norm.view(-1, 1).expand_as(input))\n",
    "\n",
    "        output = _output.view(input_size)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Crop and return a batch of cropped images from the input tensor batch using YOLOv11.\n",
    "        If no object is detected, return the original image for the respective batch item.\n",
    "\n",
    "        Args:\n",
    "            image_batch (torch.Tensor): Input tensor of shape (B, C, H, W).\n",
    "            The image batch should be normalized to \n",
    "            resnet_mean = [0.485, 0.456, 0.406]\n",
    "            resnet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of cropped images or original images if no detections.\n",
    "        \"\"\"\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "\n",
    "        avg_x = self.model.gap(x)\n",
    "        max_x = self.model.gmp(x)\n",
    "\n",
    "        x = max_x + avg_x\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.model.embedding(x)\n",
    "        \n",
    "        if self.is_norm:\n",
    "            x = self.l2_norm(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.kaiming_normal_(self.model.embedding.weight, mode='fan_out')\n",
    "        init.constant_(self.model.embedding.bias, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c3fd8c",
   "metadata": {},
   "source": [
    "## Resnet101 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7ca20e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/delta/penv/aienv/lib/python3.12/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/home/delta/penv/aienv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Resnet101(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    (gap): AdaptiveAvgPool2d(output_size=1)\n",
       "    (gmp): AdaptiveMaxPool2d(output_size=1)\n",
       "    (embedding): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet101_model = Resnet101(embedding_size=512, pretrained=True, is_norm=True, bn_freeze = True)\n",
    "\n",
    "# Path to checkpoint\n",
    "checkpoint = torch.load(resnet101_checkpoint_path)\n",
    "resnet101_model.load_state_dict(checkpoint)\n",
    "resnet101_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d87e7",
   "metadata": {},
   "source": [
    "## YOLO Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8d25a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = YOLO(yolo_checkpoint_path).to(device)  # You can specify a different model if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90961b44",
   "metadata": {},
   "source": [
    "## Batch Process image tensors with YOLO\n",
    "The function below takes a image tensor batch. The images are then cropped around the turtle head. \n",
    "If no turtle head is found the original image is used.\n",
    "\n",
    "The function returns the tensor batch, cropped around the turtles' heads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db6bbe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def batch_crop_yolov11(image_batch: torch.Tensor, model: YOLO, conf_threshold: float = 0.5, device: str = 'cuda'):\n",
    "    global error_crops\n",
    "    \"\"\"\n",
    "    Crop and return a batch of cropped images from the input tensor batch using YOLOv11.\n",
    "    If no object is detected, return the original image for the respective batch item.\n",
    "    Additionally, display the image with bounding boxes if there are more than 2 boxes.\n",
    "\n",
    "    Args:\n",
    "        image_batch (torch.Tensor): Input tensor of shape (B, C, H, W).\n",
    "        model (YOLO): Preloaded YOLOv11 model.\n",
    "        conf_threshold (float): Confidence threshold for detections.\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of cropped images or original images if no detections.\n",
    "    \"\"\"\n",
    "    # Ensure the model is on the correct device\n",
    "    model.to(device)\n",
    "\n",
    "    # Load and preprocess the image batch\n",
    "    images = image_batch.to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    results = model.predict(source=images, conf=conf_threshold, device=device, verbose=False)\n",
    "\n",
    "    # Initialize a list to store cropped images\n",
    "    cropped_images = []\n",
    "\n",
    "    # Iterate over each result\n",
    "    for result, image in zip(results, images):\n",
    "        if result.boxes is not None and len(result.boxes.xyxy) > 0:\n",
    "            boxes = result.boxes.xyxy.int().cpu().tolist()\n",
    "\n",
    "            # if len(boxes) > 2:\n",
    "            #     # Convert the image to PIL format for display\n",
    "            #     pil_image = transforms.ToPILImage()(image.cpu())\n",
    "            #     draw = ImageDraw.Draw(pil_image)\n",
    "            #     # Draw each box on the image\n",
    "            #     for box in boxes:\n",
    "            #         x_min, y_min, x_max, y_max = box\n",
    "            #         draw.rectangle([x_min, y_min, x_max, y_max], outline=\"red\", width=3)\n",
    "\n",
    "            #     # Show the image with bounding boxes\n",
    "            #     plt.imshow(pil_image)\n",
    "            #     plt.show()\n",
    "            if len(boxes) != 1:\n",
    "                error_crops = error_crops + 1\n",
    "                \n",
    "            # Crop the boxes and add them to the list\n",
    "            x_min, y_min, x_max, y_max = boxes[0]   \n",
    "            cropped_image = image[:, y_min:y_max, x_min:x_max]\n",
    "            img = resize_images_to_n_padded(cropped_image.unsqueeze(0), resize_scale)\n",
    "            cropped_images.append(img.squeeze(0))\n",
    "                \n",
    "        else:\n",
    "            # If no detections, append the original image to the batch\n",
    "            img = resize_images_to_n_padded(image.unsqueeze(0), resize_scale)\n",
    "            cropped_images.append(img.squeeze(0))\n",
    "\n",
    "    # Stack the cropped images into a single tensor\n",
    "    cropped_images_tensor = torch.stack(cropped_images)\n",
    "\n",
    "    return cropped_images_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340af22",
   "metadata": {},
   "source": [
    "## Standard Transform before Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a031037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transform(original = 256, output=224):\n",
    "    resnet_sz_resize = original\n",
    "    resnet_sz_crop = output\n",
    "    resnet_mean = [0.485, 0.456, 0.406]\n",
    "    resnet_std = [0.229, 0.224, 0.225]\n",
    "    resnet_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(resnet_sz_resize),\n",
    "        transforms.CenterCrop(resnet_sz_crop),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=resnet_mean, std=resnet_std)\n",
    "    ])\n",
    "    return resnet_transform\n",
    "resnet_transform = make_transform(original=resize_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c211bc",
   "metadata": {},
   "source": [
    "## Resize, crop and normalize tensors for embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66b44369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_embed_transform(image_tensor_batch):\n",
    "    processed_images = []\n",
    "    for image_tensor in image_tensor_batch:\n",
    "        processed_images.append(resnet_transform(image_tensor))\n",
    "    return torch.stack(processed_images)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe978b1",
   "metadata": {},
   "source": [
    "## Inference Pipeline\n",
    "Handles tensorized images in batches, applying all the inferences and transforms required to produce the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "540ce419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_pipeline(image_tensor):\n",
    "    global yolo_model, resnet101_model, error_crops\n",
    "    # Detect face in the image\n",
    "    faces_tensor = batch_crop_yolov11(image_tensor, yolo_model)\n",
    "    print(\"Number of cropping errors\", error_crops)\n",
    "    #resize the images to work for resnet\n",
    "    faces_tensor_resized = resize_images_to_n_padded(faces_tensor, n=resize_scale)\n",
    "    # crop and normalize before embedding model\n",
    "    normalized_face_tensor = pre_embed_transform(faces_tensor_resized)\n",
    "    \n",
    "    # Generate embedding for the detected face\n",
    "    embeddings = resnet101_model(normalized_face_tensor)\n",
    "    \n",
    "    return embeddings.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf8885",
   "metadata": {},
   "source": [
    "## Function to run the inference pipeline over the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66bd90a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_to_embeddings(data_loader):\n",
    "    processed_batches = []\n",
    "    print(\"Processing data...\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(data_loader, desc=\"Processing data\")):\n",
    "\n",
    "        processed_batch = infer_pipeline(inputs)\n",
    "        processed_batches.append(processed_batch.cpu())\n",
    "    return torch.stack(processed_batches).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "366d5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def turtle_similarities(query_embedding, gallery_embeddings):\n",
    "    cosine_sim = F.cosine_similarity(query_embedding, gallery_embeddings)\n",
    "    \n",
    "    return cosine_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af415e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_recognition_results(query_image, similar_images, titles):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, len(similar_images)+1, 1)\n",
    "    plt.imshow(query_image)\n",
    "    plt.title(\"Query\")\n",
    "    \n",
    "    for idx, sim_img in enumerate(similar_images):\n",
    "        plt.subplot(1, len(similar_images)+1, idx+2)\n",
    "        plt.imshow(mpimg.imread(sim_img))\n",
    "        plt.title(titles[idx])\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e1ab1",
   "metadata": {},
   "source": [
    "## Embeddings aggregator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cd8e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingAggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    Average embeddings in the batch per turtle identity to produce more accurate embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reduction : str\n",
    "        'mean' (default) or 'weighted'.\n",
    "        When 'weighted', `weights` must be supplied in forward().\n",
    "    return_index_map : bool\n",
    "        If True, also returns a tensor that maps each original sample\n",
    "        to its aggregated row  handy for gathering losses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction: str = \"mean\", return_index_map: bool = False):\n",
    "        super().__init__()\n",
    "        assert reduction in {\"mean\", \"weighted\"}\n",
    "        self.reduction = reduction\n",
    "        self.return_index_map = return_index_map\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "        self,\n",
    "        emb: torch.Tensor,             # shape (B, D)\n",
    "        labels: torch.Tensor,          # shape (B,)\n",
    "        weights: torch.Tensor | None = None\n",
    "    ):\n",
    "        device = emb.device\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # ---- gather bookkeeping -------------------------------------------------\n",
    "        # unique ids and position of each samples aggregated row\n",
    "        uniq_ids, inv = torch.unique(labels, return_inverse=True)\n",
    "        num_ids = uniq_ids.size(0)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            # quick path  use scatter_add then divide by counts\n",
    "            summed = torch.zeros(num_ids, emb.size(1), device=device).scatter_add_(0,\n",
    "                      inv.unsqueeze(-1).expand_as(emb), emb)\n",
    "            counts = torch.bincount(inv, minlength=num_ids).unsqueeze(1)\n",
    "            agg = summed / counts.clamp_min(1)\n",
    "\n",
    "        else:  # weighted mean\n",
    "            assert weights is not None, \"`weights` required for weighted reduction\"\n",
    "            weights = weights.to(device).unsqueeze(1)  # (B, 1)\n",
    "            w_sum = torch.zeros(num_ids, 1, device=device).scatter_add_(0, inv.unsqueeze(-1), weights)\n",
    "            w_emb = torch.zeros_like(summed).scatter_add_(0, inv.unsqueeze(-1).expand_as(emb), emb * weights)\n",
    "            agg = w_emb / w_sum.clamp_min(1e-8)\n",
    "\n",
    "        if self.return_index_map:\n",
    "            return agg, uniq_ids, inv\n",
    "        return agg, uniq_ids, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6665ca3",
   "metadata": {},
   "source": [
    "## Sort embeddings by similarity with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbcb8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_em_by_sim(similarities, query_labels):\n",
    "    similarities_and_labels = [(similarity.item(), query_labels[i].item()) for i, similarity in enumerate(similarities)]\n",
    "\n",
    "    # Sort the list by similarity in descending order\n",
    "    similarities_and_labels_sorted = sorted(similarities_and_labels, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Optionally, separate the sorted similarities and labels back into two lists/tensors\n",
    "    sorted_similarities = [pair[0] for pair in similarities_and_labels_sorted]\n",
    "    sorted_labels = [pair[1] for pair in similarities_and_labels_sorted]\n",
    "\n",
    "    return torch.tensor(sorted_similarities), torch.tensor(sorted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b6a06c",
   "metadata": {},
   "source": [
    "## Retrieve the images in a dataset that match the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5760ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_indices(string_list, query):\n",
    "    # Create a list of indices where the string matches the query\n",
    "    matching_indices = [i for i, s in enumerate(string_list) if s == query]\n",
    "    return matching_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be01ec",
   "metadata": {},
   "source": [
    "## Basic tensor transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9cc593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109405ba",
   "metadata": {},
   "source": [
    "## Main Pipeline Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7830ed18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 8396\n",
      "Data length: 8396\n",
      "Processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   0%|          | 0/1050 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   0%|          | 1/1050 [00:02<42:15,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   0%|          | 2/1050 [00:03<25:25,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   0%|          | 4/1050 [00:04<18:10,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   0%|          | 5/1050 [00:05<16:15,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   1%|          | 6/1050 [00:06<14:51,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 17\n",
      "Number of cropping errors 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   1%|          | 8/1050 [00:07<14:05,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   1%|          | 9/1050 [00:08<13:37,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   1%|          | 10/1050 [00:09<13:48,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   1%|          | 11/1050 [00:10<15:14,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   1%|          | 12/1050 [00:11<14:56,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 40\n",
      "Number of cropping errors 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   1%|         | 14/1050 [00:12<14:37,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   1%|         | 15/1050 [00:13<14:50,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   2%|         | 16/1050 [00:14<14:46,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   2%|         | 17/1050 [00:15<15:43,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   2%|         | 18/1050 [00:16<17:13,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   2%|         | 19/1050 [00:17<16:09,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   2%|         | 20/1050 [00:18<17:18,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   2%|         | 21/1050 [00:19<16:50,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   2%|         | 22/1050 [00:20<17:09,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   2%|         | 23/1050 [00:21<17:11,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   2%|         | 24/1050 [00:22<17:01,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   2%|         | 25/1050 [00:23<16:58,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   2%|         | 26/1050 [00:24<17:35,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   3%|         | 27/1050 [00:25<16:06,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   3%|         | 28/1050 [00:26<15:12,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   3%|         | 29/1050 [00:27<14:17,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   3%|         | 30/1050 [00:28<14:49,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   3%|         | 31/1050 [00:29<19:23,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   3%|         | 32/1050 [00:30<18:18,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   3%|         | 33/1050 [00:32<19:08,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   3%|         | 34/1050 [00:33<19:29,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   3%|         | 35/1050 [00:34<19:38,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   3%|         | 36/1050 [00:35<20:12,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   4%|         | 37/1050 [00:37<21:07,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   4%|         | 38/1050 [00:38<21:34,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   4%|         | 39/1050 [00:39<21:55,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   4%|         | 40/1050 [00:41<22:04,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   4%|         | 41/1050 [00:42<21:49,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   4%|         | 42/1050 [00:43<19:30,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   4%|         | 43/1050 [00:44<20:17,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   4%|         | 44/1050 [00:45<20:09,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   4%|         | 45/1050 [00:47<20:42,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   4%|         | 46/1050 [00:48<20:29,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 153\n",
      "Number of cropping errors 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   5%|         | 48/1050 [00:50<19:48,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   5%|         | 49/1050 [00:51<20:38,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   5%|         | 50/1050 [00:53<21:19,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   5%|         | 51/1050 [00:54<20:20,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   5%|         | 52/1050 [00:55<19:57,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   5%|         | 53/1050 [00:57<21:52,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   5%|         | 54/1050 [00:58<21:59,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   5%|         | 55/1050 [00:59<22:15,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   5%|         | 56/1050 [01:01<24:30,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 173\n",
      "Number of cropping errors 174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   6%|         | 58/1050 [01:03<18:54,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   6%|         | 59/1050 [01:04<16:25,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   6%|         | 60/1050 [01:05<16:31,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   6%|         | 61/1050 [01:06<20:13,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   6%|         | 62/1050 [01:07<17:55,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   6%|         | 63/1050 [01:08<17:35,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   6%|         | 64/1050 [01:09<15:42,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   6%|         | 65/1050 [01:10<14:27,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 200\n",
      "Number of cropping errors 204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   6%|         | 66/1050 [01:10<13:34,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   6%|         | 67/1050 [01:11<13:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   7%|         | 69/1050 [01:12<12:52,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 218\n",
      "Number of cropping errors 221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   7%|         | 70/1050 [01:14<14:36,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   7%|         | 72/1050 [01:16<15:37,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   7%|         | 73/1050 [01:17<15:34,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   7%|         | 74/1050 [01:18<16:18,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   7%|         | 75/1050 [01:19<15:40,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   7%|         | 76/1050 [01:20<16:26,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   7%|         | 77/1050 [01:21<17:14,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   7%|         | 78/1050 [01:22<18:42,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   8%|         | 79/1050 [01:24<20:14,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   8%|         | 80/1050 [01:25<20:56,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 253\n",
      "Number of cropping errors 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   8%|         | 82/1050 [01:28<21:43,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   8%|         | 83/1050 [01:29<22:44,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   8%|         | 84/1050 [01:31<22:51,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   8%|         | 85/1050 [01:32<22:14,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cropping errors 273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   8%|         | 85/1050 [01:33<17:45,  1.10s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TurtlesDataset(root\u001b[38;5;241m=\u001b[39mdataset_path, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39mto_tensor_transform)\n\u001b[1;32m     10\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39minference_batch_size, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m dataset_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_images_to_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m dataset_labels \u001b[38;5;241m=\u001b[39m get_dataloader_labels(data_loader)\n\u001b[1;32m     13\u001b[0m torch\u001b[38;5;241m.\u001b[39msave((dataset_embeddings, dataset_labels), saved_embeddings_path)\n",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m, in \u001b[0;36mprocess_images_to_embeddings\u001b[0;34m(data_loader)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(data_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing data\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m----> 6\u001b[0m     processed_batch \u001b[38;5;241m=\u001b[39m \u001b[43minfer_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     processed_batches\u001b[38;5;241m.\u001b[39mappend(processed_batch\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(processed_batches)\u001b[38;5;241m.\u001b[39mcpu()\n",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m, in \u001b[0;36minfer_pipeline\u001b[0;34m(image_tensor)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m yolo_model, resnet101_model, error_crops\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Detect face in the image\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m faces_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_crop_yolov11\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myolo_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of cropping errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, error_crops)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#resize the images to work for resnet\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m, in \u001b[0;36mbatch_crop_yolov11\u001b[0;34m(image_batch, model, conf_threshold, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m images \u001b[38;5;241m=\u001b[39m image_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Perform inference\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Initialize a list to store cropped images\u001b[39;00m\n\u001b[1;32m     32\u001b[0m cropped_images \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/ultralytics/engine/model.py:549\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:218\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:329\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 329\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:173\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    169\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    172\u001b[0m )\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/ultralytics/nn/autobackend.py:591\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 591\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:120\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:138\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:159\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 159\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    160\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/ultralytics/nn/modules/head.py:75\u001b[0m, in \u001b[0;36mDetect.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:  \u001b[38;5;66;03m# Training path\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m---> 75\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport \u001b[38;5;28;01melse\u001b[39;00m (y, x)\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/ultralytics/nn/modules/head.py:117\u001b[0m, in \u001b[0;36mDetect._inference\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    115\u001b[0m x_cat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([xi\u001b[38;5;241m.\u001b[39mview(shape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m x], \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m shape):\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchors, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmake_anchors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m shape\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtflite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgetpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtfjs\u001b[39m\u001b[38;5;124m\"\u001b[39m}:  \u001b[38;5;66;03m# avoid TF FlexSplitV ops\u001b[39;00m\n",
      "File \u001b[0;32m~/penv/aienv/lib/python3.12/site-packages/ultralytics/utils/tal.py:375\u001b[0m, in \u001b[0;36mmake_anchors\u001b[0;34m(feats, strides, grid_cell_offset)\u001b[0m\n\u001b[1;32m    373\u001b[0m     sy, sx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmeshgrid(sy, sx, indexing\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mij\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_10 \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmeshgrid(sy, sx)\n\u001b[1;32m    374\u001b[0m     anchor_points\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mstack((sx, sy), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m--> 375\u001b[0m     stride_tensor\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(anchor_points), torch\u001b[38;5;241m.\u001b[39mcat(stride_tensor)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings_exist = os.path.exists(saved_embeddings_path)\n",
    "error_crops = 0\n",
    "# Load dataset embeddings\n",
    "dataset_embeddings = None\n",
    "if embeddings_exist:\n",
    "    dataset_embeddings, dataset_labels = torch.load(saved_embeddings_path)\n",
    "else:\n",
    "    generate_data_folder_metadata(dataset_path)\n",
    "    dataset = TurtlesDataset(root=dataset_path, mode=\"all\", transform=to_tensor_transform)\n",
    "    data_loader = DataLoader(dataset, batch_size=inference_batch_size, pin_memory=True, shuffle=False, num_workers=1)\n",
    "    dataset_embeddings = process_images_to_embeddings(data_loader)\n",
    "    dataset_labels = get_dataloader_labels(data_loader)\n",
    "    torch.save((dataset_embeddings, dataset_labels), saved_embeddings_path)\n",
    "\n",
    "\n",
    "# Load query embeddings\n",
    "query_metadata = generate_query_folder_metadata(query_folder_path)\n",
    "query_dataset = TurtlesDataset(data=query_metadata, mode=\"all\", transform=to_tensor_transform)\n",
    "query_loader = DataLoader(query_dataset, batch_size=inference_batch_size, pin_memory=True, shuffle=False, num_workers=1)\n",
    "query_embeddings = process_images_to_embeddings(query_loader)\n",
    "query_labels = get_dataloader_labels(query_loader)\n",
    "\n",
    "# Aggregate embeddings by identity if enabled\n",
    "aggregator = EmbeddingAggregator().to(device)\n",
    "if aggregate_dataset_identities:\n",
    "    dataset_embeddings, dataset_labels, _ = aggregator(dataset_embeddings, labels=dataset_labels)\n",
    "if aggregate_query_identities:\n",
    "    query_embeddings, query_labels, _ = aggregator(query_embeddings, labels=query_labels)\n",
    "    \n",
    "print(\"Comparing embeddings...\")\n",
    "for query_index in range(query_embeddings.shape[0]):\n",
    "    query_em = query_embeddings[query_index]\n",
    "    query_label = query_labels[query_index]\n",
    "    \n",
    "    similarities = turtle_similarities(query_em, dataset_embeddings)\n",
    "    sorted_similarities, sorted_labels = sort_em_by_sim(similarities, dataset_labels)[:top_n]\n",
    "    images = []\n",
    "    images_labels = []\n",
    "    for label in sorted_labels:\n",
    "        image_index = find_matching_indices(dataset_labels, label)[0]\n",
    "        \n",
    "        image = dataset[image_index]\n",
    "        images.append(image)\n",
    "        images_labels.append(label)\n",
    "    query_image_index = find_matching_indices(query_labels, query_label)[0]\n",
    "    query_image = query_dataset[query_image_index]\n",
    "    plot_recognition_results(query_image, images, images_labels)\n",
    "     \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
